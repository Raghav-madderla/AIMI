{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub -q"
      ],
      "metadata": {
        "id": "QKu9cnEP5ZzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdBRz7OR45DE",
        "outputId": "b8aabbff-8c4e-4767-b9c7-2aeada702041"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîπ Generating Reference for domain 'SQL'...\n",
            "‚úÖ Reference Generated:\n",
            "You are an expert in SQL.\n",
            "Write a concise, technically perfect answer to the following interview question.\n",
            "Focus on the definition and the 'why'. Do NOT use code examples unless absolutely necessary.\n",
            "\n",
            "Question: Explain how a database Index works.\n",
            "\n",
            "Answer: \n",
            "\n",
            "A database index is a data structure that improves the speed of data retrieval operations by providing a quick way to locate data. It's essentially a pointer to a specific location in the database table, allowing the database management system (DBMS) to quickly locate and retrieve the required data.\n",
            "\n",
            "Think of an index like the index in a book. Just as the index in a book allows you to quickly find a specific chapter or section, a database index enables the DBMS to quickly locate a specific row or set of rows in a table.\n",
            "\n",
            "The index is created on one or more columns of a table, and it contains a copy of the indexed column(s) along with a unique identifier, called the row ID or RID, that points to the actual location of the row in the table. When a query is executed, the DBMS uses the index to quickly locate the required data, rather than scanning the entire table.\n",
            "\n",
            "The main reasons for using indexes are:\n",
            "\n",
            "1.  **Improved query performance**: Indexes enable the DBMS to quickly locate data, reducing the time it takes to execute queries.\n",
            "2.  **Reduced disk I/O**: By allowing the DBMS to quickly locate data, indexes reduce the need for disk I/O operations, which can be time-consuming.\n",
            "3\n",
            "\n",
            "----------------------------------------\n",
            "üîπ Running Judge...\n",
            "\n",
            "--- üìä FINAL JSON OUTPUT ---\n",
            "‚ùå Failed to parse JSON. Raw output:\n",
            "You are a strict technical interviewer.\n",
            "\n",
            "### Question:\n",
            "Explain how a database Index works.\n",
            "\n",
            "### Reference Answer (Truth):\n",
            "You are an expert in SQL.\n",
            "Write a concise, technically perfect answer to the following interview question.\n",
            "Focus on the definition and the 'why'. Do NOT use code examples unless absolutely necessary.\n",
            "\n",
            "Question: Explain how a database Index works.\n",
            "\n",
            "Answer: \n",
            "\n",
            "A database index is a data structure that improves the speed of data retrieval operations by providing a quick way to locate data. It's essentially a pointer to a specific location in the database table, allowing the database management system (DBMS) to quickly locate and retrieve the required data.\n",
            "\n",
            "Think of an index like the index in a book. Just as the index in a book allows you to quickly find a specific chapter or section, a database index enables the DBMS to quickly locate a specific row or set of rows in a table.\n",
            "\n",
            "The index is created on one or more columns of a table, and it contains a copy of the indexed column(s) along with a unique identifier, called the row ID or RID, that points to the actual location of the row in the table. When a query is executed, the DBMS uses the index to quickly locate the required data, rather than scanning the entire table.\n",
            "\n",
            "The main reasons for using indexes are:\n",
            "\n",
            "1.  **Improved query performance**: Indexes enable the DBMS to quickly locate data, reducing the time it takes to execute queries.\n",
            "2.  **Reduced disk I/O**: By allowing the DBMS to quickly locate data, indexes reduce the need for disk I/O operations, which can be time-consuming.\n",
            "3\n",
            "\n",
            "### Candidate's Answer:\n",
            "An index is a data structure, similar to a book's index, that improves the speed of data retrieval operations. It allows the database engine to find rows much faster than scanning the entire table.\n",
            "\n",
            "### Evaluation Protocol:\n",
            "1. **Analyze:** Compare the Candidate's answer to the Reference. Note matches and misses.\n",
            "2. **Score Technical Accuracy (0.0-1.0):** Is the information factually correct? (No lies/hallucinations).\n",
            "3. **Score Completeness (0.0-1.0):** Did they cover the main points? (e.g. missed \"test data\" in overfitting).\n",
            "4. **Score Clarity (0.0-1.0):** Is the answer easy to understand?\n",
            "5. **Overall Score (0.0-1.0):** A weighted average of the above.\n",
            "\n",
            "### Instructions:\n",
            "- Be objective.\n",
            "- **CRITICAL:** Respond using ONLY valid JSON. Do not write anything else.\n",
            "\n",
            "### Output Format (JSON):\n",
            "{\n",
            "    \"analysis\": \"<Short comparison of Reference vs Candidate>\",\n",
            "    \"technical_accuracy\": <float>,\n",
            "    \"completeness\": <float>,\n",
            "    \"clarity\": <float>,\n",
            "    \"overall_score\": <float>,\n",
            "    \"feedback\": \"<Constructive feedback for the student>\"\n",
            "}\n",
            "\n",
            "### Response:\n",
            "{\n",
            "    \"analysis\": \"The candidate's answer is mostly correct, but lacks some details. They correctly explained the purpose of an index and its relation to a book's index. However, they didn't mention the unique identifier (row ID or RID) that points to the actual location of the row in the table.\",\n",
            "    \"technical_accuracy\": 0.8,\n",
            "    \"completeness\": 0.6,\n",
            "    \"clarity\": 0.9,\n",
            "    \"overall_score\": 0.8,\n",
            "    \"feedback\": \"Great job on explaining the purpose of an index! To improve, make sure to include all the key details, such as the unique identifier (row ID or RID). Also, try to provide more examples or analogies to help illustrate your points.\"\n",
            "}  ### End of Response ###\n",
            "\n",
            "### Evaluation:\n",
            "The candidate's answer is mostly correct, but lacks some details. They correctly explained the purpose of an index and its relation to a book's index. However, they didn't mention the unique identifier (row ID or RID) that points to the actual location of the row in the table.\n",
            "\n",
            "The overall score is 0.8, indicating a good understanding of the topic, but room for improvement in terms of completeness and technical accuracy.\n",
            "\n",
            "The feedback suggests that the candidate should include all the key details and provide more examples or analogies to help illustrate their points. This will help them to better communicate their understanding of the topic and provide a more comprehensive answer.  ### End of Evaluation ###\n",
            "\n",
            "### Next Steps:\n",
            "- Review the feedback and work on improving the completeness and technical accuracy of the answer.\n",
            "- Practice explaining the concept of an index in more detail, including the unique identifier (row ID or RID).\n",
            "- Consider providing more examples or analogies to help illustrate the points.  ### End of Next Steps ###  ### End of Response ###  ### End of Evaluation ###  ### End of Next Steps ###  ### End of Response ###  ### End of Evaluation ###  ### End of Next Steps ###  ### End of Response ###  ### End of Evaluation ###  ### End of Next Steps ###  ### End of Response ###  ### End of Evaluation ###  ### End of Next Steps ###  ### End of Response ###  ### End of Evaluation ###  ### End of Next Steps ###  ### End of Response ###  ### End of Evaluation ###  ### End of Next Steps ###  ### End of Response ###  ### End of Evaluation ###  ### End of Next Steps ###  ### End of Response ### \n"
          ]
        }
      ],
      "source": [
        "# @title Test Answer Evaluation Logic (Inference Endpoint)\n",
        "\n",
        "import json\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "ENDPOINT_URL = \"\"\n",
        "HF_TOKEN = \"\"\n",
        "\n",
        "# Initialize the Client\n",
        "client = InferenceClient(base_url=ENDPOINT_URL, token=HF_TOKEN)\n",
        "\n",
        "# --- 1. Define the Test Case ---\n",
        "TEST_DOMAIN = \"SQL\"\n",
        "TEST_QUESTION = \"Explain how a database Index works.\"\n",
        "TEST_USER_ANSWER = \"An index is a data structure, similar to a book's index, that improves the speed of data retrieval operations. It allows the database engine to find rows much faster than scanning the entire table.\"\n",
        "\n",
        "# --- 2. Step A: Generate Reference (Expert) ---\n",
        "# Using your exact prompt structure\n",
        "reference_prompt = f\"\"\"You are an expert in {TEST_DOMAIN}.\n",
        "Write a concise, technically perfect answer to the following interview question.\n",
        "Focus on the definition and the 'why'. Do NOT use code examples unless absolutely necessary.\n",
        "\n",
        "Question: {TEST_QUESTION}\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "print(f\"üîπ Generating Reference for domain '{TEST_DOMAIN}'...\")\n",
        "\n",
        "try:\n",
        "    # Call the endpoint\n",
        "    gold_standard = client.text_generation(\n",
        "        prompt=reference_prompt,\n",
        "        max_new_tokens=256,\n",
        "        temperature=0.2,\n",
        "        stop_sequences=[\"<|end_of_text|>\"] # Stop if model finishes early\n",
        "    ).strip()\n",
        "\n",
        "    print(f\"‚úÖ Reference Generated:\\n{gold_standard}\\n\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in Step A: {e}\")\n",
        "    gold_standard = \"ERROR GENERATING REFERENCE\"\n",
        "\n",
        "# --- 3. Step B: Multi-Metric Judge ---\n",
        "# Using your exact prompt structure\n",
        "judge_prompt = f\"\"\"You are a strict technical interviewer.\n",
        "\n",
        "### Question:\n",
        "{TEST_QUESTION}\n",
        "\n",
        "### Reference Answer (Truth):\n",
        "{gold_standard}\n",
        "\n",
        "### Candidate's Answer:\n",
        "{TEST_USER_ANSWER}\n",
        "\n",
        "### Evaluation Protocol:\n",
        "1. **Analyze:** Compare the Candidate's answer to the Reference. Note matches and misses.\n",
        "2. **Score Technical Accuracy (0.0-1.0):** Is the information factually correct? (No lies/hallucinations).\n",
        "3. **Score Completeness (0.0-1.0):** Did they cover the main points? (e.g. missed \"test data\" in overfitting).\n",
        "4. **Score Clarity (0.0-1.0):** Is the answer easy to understand?\n",
        "5. **Overall Score (0.0-1.0):** A weighted average of the above.\n",
        "\n",
        "### Instructions:\n",
        "- Be objective.\n",
        "- **CRITICAL:** Respond using ONLY valid JSON. Do not write anything else.\n",
        "\n",
        "### Output Format (JSON):\n",
        "{{\n",
        "    \"analysis\": \"<Short comparison of Reference vs Candidate>\",\n",
        "    \"technical_accuracy\": <float>,\n",
        "    \"completeness\": <float>,\n",
        "    \"clarity\": <float>,\n",
        "    \"overall_score\": <float>,\n",
        "    \"feedback\": \"<Constructive feedback for the student>\"\n",
        "}}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "print(\"üîπ Running Judge...\")\n",
        "\n",
        "try:\n",
        "    # Call the endpoint with lower temperature for JSON stability\n",
        "    response_text = client.text_generation(\n",
        "        prompt=judge_prompt,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.1,\n",
        "        stop_sequences=[\"<|end_of_text|>\"]\n",
        "    )\n",
        "\n",
        "    # --- 4. Parse and Display Results ---\n",
        "    print(\"\\n--- üìä FINAL JSON OUTPUT ---\")\n",
        "\n",
        "    # Clean up potential markdown code blocks (Llama 3 loves to add these)\n",
        "    clean_json = response_text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "\n",
        "    data = json.loads(clean_json)\n",
        "\n",
        "    print(json.dumps(data, indent=4))\n",
        "\n",
        "except json.JSONDecodeError:\n",
        "    print(\"‚ùå Failed to parse JSON. Raw output:\")\n",
        "    print(response_text)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in Step B: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2"
      ],
      "metadata": {
        "id": "l7DR34n66dj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q google-generativeai huggingface_hub numpy"
      ],
      "metadata": {
        "id": "h6BDhHnS6fab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "import numpy as np\n",
        "import google.generativeai as genai\n",
        "from huggingface_hub import InferenceClient\n",
        "import time\n",
        "import re\n",
        "import ast\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "HF_ENDPOINT_URL = \"\"\n",
        "HF_TOKEN = \"\"\n",
        "GEMINI_API_KEY = \"\"\n",
        "\n",
        "# Configure Clients\n",
        "hf_client = InferenceClient(base_url=HF_ENDPOINT_URL, token=HF_TOKEN)\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "# --- 2. HELPER FUNCTIONS ---\n",
        "def calculate_perplexity(response_details):\n",
        "    tokens = response_details.tokens\n",
        "    if not tokens: return 0.0\n",
        "    log_probs = [t.logprob for t in tokens if t.logprob is not None]\n",
        "    if not log_probs: return 0.0\n",
        "    return math.exp(-np.mean(log_probs))\n",
        "\n",
        "def clean_and_parse_json(text):\n",
        "    try:\n",
        "        # Regex to find the largest outer block enclosed in {}\n",
        "        match = re.search(r\"\\{[\\s\\S]*\\}\", text)\n",
        "        if match:\n",
        "            json_str = match.group(0)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            return json.loads(json_str)\n",
        "        except json.JSONDecodeError:\n",
        "            pass\n",
        "\n",
        "        return ast.literal_eval(json_str)\n",
        "    except Exception as e:\n",
        "        return None\n",
        "\n",
        "def gemini_meta_judge(domain, question, user_answer, model_feedback_json):\n",
        "    model = genai.GenerativeModel('gemini-2.5-flash-lite')\n",
        "\n",
        "    if model_feedback_json is None:\n",
        "        llama_feedback_str = \"Error: Llama model failed to generate valid JSON structure.\"\n",
        "    else:\n",
        "        llama_feedback_str = json.dumps(model_feedback_json)\n",
        "\n",
        "    # Simplified Rubric for robustness\n",
        "    prompt = f\"\"\"You are a 'Meta-Evaluator'.\n",
        "\n",
        "    Scenario:\n",
        "    - Domain: {domain}\n",
        "    - Question: {question}\n",
        "    - Candidate Answer: {user_answer}\n",
        "\n",
        "    AI Feedback to Evaluate:\n",
        "    {llama_feedback_str}\n",
        "\n",
        "    Task: Evaluate the AI Feedback. Return ONLY JSON:\n",
        "    {{\n",
        "        \"relevance_score\": int (1-5), \"relevance_reason\": \"string\",\n",
        "        \"faithfulness_score\": int (1-5), \"faithfulness_reason\": \"string\",\n",
        "        \"helpfulness_score\": int (1-5), \"helpfulness_reason\": \"string\",\n",
        "        \"coherence_score\": int (1-5), \"coherence_reason\": \"string\",\n",
        "        \"overall_score\": float\n",
        "    }}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        clean_text = response.text.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "        return json.loads(clean_text)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# --- 3. TEST CASES ---\n",
        "test_cases = [\n",
        "    {\n",
        "        \"domain\": \"SQL\",\n",
        "        \"question\": \"What is the difference between TRUNCATE and DELETE commands?\",\n",
        "        \"user_answer\": \"TRUNCATE is a DDL command that removes all rows and resets identity. DELETE is DML and can use a WHERE clause. TRUNCATE is generally faster.\"\n",
        "    },\n",
        "    {\n",
        "        \"domain\": \"Python\",\n",
        "        \"question\": \"Explain the concept of decorators and provide a use case.\",\n",
        "        \"user_answer\": \"Decorators are used to decorate functions. They make the code look pretty. I use them for styling.\"\n",
        "    },\n",
        "    {\n",
        "        \"domain\": \"Machine Learning\",\n",
        "        \"question\": \"Describe the Bias-Variance tradeoff.\",\n",
        "        \"user_answer\": \"High bias means underfitting. High variance means overfitting. We want to find a balance to minimize total error.\"\n",
        "    },\n",
        "    {\n",
        "        \"domain\": \"System Design\",\n",
        "        \"question\": \"How would you design a rate limiter?\",\n",
        "        \"user_answer\": \"I would use a database to count requests. Every time a user hits the API, I update the row. If it's above 10, I block them.\"\n",
        "    },\n",
        "    {\n",
        "        \"domain\": \"Data Engineering\",\n",
        "        \"question\": \"Explain the difference between ETL and ELT.\",\n",
        "        \"user_answer\": \"ETL stands for Extract Transform Load. ELT stands for Extract Load Transform. In ELT, we load raw data into the warehouse first, then transform it using the warehouse's power (like Snowflake).\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# --- 4. EXECUTION ---\n",
        "ppl_scores = []\n",
        "gemini_scores = []\n",
        "llama_scores = []\n",
        "\n",
        "print(f\"Starting Detailed Benchmark...\\n\")\n",
        "\n",
        "for i, test in enumerate(test_cases):\n",
        "    print(f\"Test {i+1}: {test['domain']}\")\n",
        "\n",
        "    # --- A. Generate Reference ---\n",
        "    # Removed <|begin_of_text|> to avoid double BOS token issues\n",
        "    ref_prompt = f\"\"\"<|start_header_id|>system<|end_header_id|>\\n\\nYou are an expert in {test['domain']}. Write a concise, technically perfect answer.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n{test['question']}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\"\"\n",
        "\n",
        "    try:\n",
        "        ref_response = hf_client.text_generation(\n",
        "            prompt=ref_prompt,\n",
        "            max_new_tokens=200,\n",
        "            stop_sequences=[\"<|eot_id|>\"],\n",
        "            return_full_text=False # <--- FIX 1\n",
        "        )\n",
        "        gold_standard = ref_response.strip()\n",
        "    except:\n",
        "        gold_standard = \"Error.\"\n",
        "\n",
        "    # --- B. Run Llama Judge ---\n",
        "    # Removed <|begin_of_text|> and added return_full_text=False\n",
        "    judge_prompt = f\"\"\"<|start_header_id|>system<|end_header_id|>\\n\\nYou are a strict technical interviewer. Compare the Candidate's Answer to the Reference.\n",
        "Return ONLY valid JSON with fields: \"analysis\", \"technical_accuracy\" (0.0-1.0), \"completeness\" (0.0-1.0), \"overall_score\" (0.0-1.0), \"feedback\".<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nQuestion: {test['question']}\n",
        "Reference: {gold_standard}\n",
        "Candidate Answer: {test['user_answer']}\n",
        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = hf_client.text_generation(\n",
        "            prompt=judge_prompt,\n",
        "            max_new_tokens=400,\n",
        "            temperature=0.1,\n",
        "            details=True,\n",
        "            stop_sequences=[\"<|eot_id|>\"],\n",
        "            return_full_text=False # <--- FIX 2: Crucial for parsing\n",
        "        )\n",
        "\n",
        "        # 1. Perplexity\n",
        "        ppl = calculate_perplexity(response.details)\n",
        "        ppl_scores.append(ppl)\n",
        "\n",
        "        # 2. Parse Llama Output\n",
        "        llama_json = clean_and_parse_json(response.generated_text)\n",
        "\n",
        "        if llama_json:\n",
        "            # Scale 0-1 score to 0-5\n",
        "            raw_score = float(llama_json.get('overall_score', 0.0))\n",
        "            l_score = raw_score * 5.0\n",
        "            llama_scores.append(l_score)\n",
        "            print(f\"   Llama Awarded Score: {l_score:.2f} / 5.0\")\n",
        "        else:\n",
        "            print(f\"   Llama output invalid JSON\")\n",
        "            print(f\"   RAW OUTPUT SNAPSHOT: {response.generated_text[:100]}...\") # Debug print\n",
        "            llama_scores.append(0.0)\n",
        "\n",
        "        # 3. Gemini Meta-Review\n",
        "        gemini_json = gemini_meta_judge(test['domain'], test['question'], test['user_answer'], llama_json)\n",
        "\n",
        "        if gemini_json:\n",
        "            g_score = gemini_json.get('overall_score', 0)\n",
        "            gemini_scores.append(g_score)\n",
        "\n",
        "            print(\"   Gemini Evaluation:\")\n",
        "            print(f\"      ‚Ä¢ Relevance:    {gemini_json.get('relevance_score')}/5\")\n",
        "            print(f\"      ‚Ä¢ Faithfulness: {gemini_json.get('faithfulness_score')}/5\")\n",
        "            print(f\"      ‚Ä¢ Helpfulness:  {gemini_json.get('helpfulness_score')}/5\")\n",
        "            print(f\"      ‚Ä¢ Coherence:    {gemini_json.get('coherence_score')}/5\")\n",
        "            print(f\"      ‚Ä¢ Overall:      {g_score}/5\")\n",
        "        else:\n",
        "            print(\"      Gemini failed to evaluate\")\n",
        "\n",
        "        print(f\"   üìâ PPL: {ppl:.4f}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   Error: {e}\")\n",
        "\n",
        "# --- 5. FINAL REPORT ---\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"FINAL BENCHMARK REPORT\")\n",
        "print(\"=\"*30)\n",
        "if ppl_scores: print(f\"Avg Perplexity:        {np.mean(ppl_scores):.4f}\")\n",
        "if llama_scores: print(f\"Avg Llama Awarded:     {np.mean(llama_scores):.2f} / 5.0\")\n",
        "if gemini_scores: print(f\"Avg Gemini Meta-Score: {np.mean(gemini_scores):.2f} / 5.0\")\n",
        "print(\"=\"*30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EO84BGZbrdsA",
        "outputId": "27051916-676e-4dce-cd07-e5004dcf5fc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Detailed Benchmark...\n",
            "\n",
            "Test 1: SQL\n",
            "   Llama Awarded Score: 3.50 / 5.0\n",
            "   Gemini Evaluation:\n",
            "      ‚Ä¢ Relevance:    5/5\n",
            "      ‚Ä¢ Faithfulness: 5/5\n",
            "      ‚Ä¢ Helpfulness:  4/5\n",
            "      ‚Ä¢ Coherence:    5/5\n",
            "      ‚Ä¢ Overall:      4.5/5\n",
            "   üìâ PPL: 1.0412\n",
            "--------------------------------------------------\n",
            "Test 2: Python\n",
            "   Llama Awarded Score: 0.00 / 5.0\n",
            "   Gemini Evaluation:\n",
            "      ‚Ä¢ Relevance:    5/5\n",
            "      ‚Ä¢ Faithfulness: 5/5\n",
            "      ‚Ä¢ Helpfulness:  5/5\n",
            "      ‚Ä¢ Coherence:    5/5\n",
            "      ‚Ä¢ Overall:      5.0/5\n",
            "   üìâ PPL: 1.0521\n",
            "--------------------------------------------------\n",
            "Test 3: Machine Learning\n",
            "   Llama Awarded Score: 2.50 / 5.0\n",
            "   Gemini Evaluation:\n",
            "      ‚Ä¢ Relevance:    5/5\n",
            "      ‚Ä¢ Faithfulness: 5/5\n",
            "      ‚Ä¢ Helpfulness:  4/5\n",
            "      ‚Ä¢ Coherence:    5/5\n",
            "      ‚Ä¢ Overall:      4.5/5\n",
            "   üìâ PPL: 1.0245\n",
            "--------------------------------------------------\n",
            "Test 4: System Design\n",
            "   Llama Awarded Score: 0.00 / 5.0\n",
            "   Gemini Evaluation:\n",
            "      ‚Ä¢ Relevance:    5/5\n",
            "      ‚Ä¢ Faithfulness: 4/5\n",
            "      ‚Ä¢ Helpfulness:  4/5\n",
            "      ‚Ä¢ Coherence:    5/5\n",
            "      ‚Ä¢ Overall:      4.5/5\n",
            "   üìâ PPL: 1.0333\n",
            "--------------------------------------------------\n",
            "Test 5: Data Engineering\n",
            "   Llama Awarded Score: 2.50 / 5.0\n",
            "   Gemini Evaluation:\n",
            "      ‚Ä¢ Relevance:    5/5\n",
            "      ‚Ä¢ Faithfulness: 5/5\n",
            "      ‚Ä¢ Helpfulness:  4/5\n",
            "      ‚Ä¢ Coherence:    5/5\n",
            "      ‚Ä¢ Overall:      4.75/5\n",
            "   üìâ PPL: 1.0456\n",
            "--------------------------------------------------\n",
            "\n",
            "==============================\n",
            "FINAL BENCHMARK REPORT\n",
            "==============================\n",
            "Avg Perplexity:        1.0393\n",
            "Avg Llama Awarded:     1.70 / 5.0\n",
            "Avg Gemini Meta-Score: 4.65 / 5.0\n",
            "==============================\n"
          ]
        }
      ]
    }
  ]
}